{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuF_sZeBYA7i"
      },
      "outputs": [],
      "source": [
        "# Connecting to google drive to access image dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "import cv2\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import statistics\n",
        "import h5py\n",
        "import zipfile\n",
        "import gc\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix"
      ],
      "metadata": {
        "id": "ZTmQsr-GlhOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the google drive path, where the images dataset(zip file) is present.\n",
        "# We will extract the data from the zip file in google colab locally, to process the data fastly.\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/Colab-Notebooks/CMIL-Assessment.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/dataset\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "ELhPwTUTnmQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4572ODLTYA7j"
      },
      "outputs": [],
      "source": [
        "# Function to convert every image present in a folder (with path \"files_path\") to a vector\n",
        "def image_vector(files_path):\n",
        "    # Storing all the names of image files (present at \"files_path\" location) in \"files\"\n",
        "    files = os.listdir(files_path)\n",
        "    # Appending the \"files_path\" name along with the names of image files present in \"files\"\n",
        "    files_complete_path = [files_path + i for i in files]\n",
        "    image_vector_list = []\n",
        "    for i in files_complete_path:\n",
        "        image = cv2.imread(i)\n",
        "        # Convert BGR (default in OpenCV) to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_vector_list.append(image_rgb)\n",
        "        # Appending Shape of the Vector of all the Images\n",
        "    image_shapes = [i.shape for i in image_vector_list]\n",
        "    return image_vector_list, image_shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMSdMb2aYA7j"
      },
      "outputs": [],
      "source": [
        "# Function for Resizing and Padding images while retaining maximum information\n",
        "def reshape_image_size(vector_list, target_height, target_width):\n",
        "    # target_size is a list of 2 numbers\n",
        "    list_copy = []\n",
        "    for vector in vector_list:\n",
        "        height, width = vector.shape[0], vector.shape[1]\n",
        "        # Resize the image based on target_size:\n",
        "        if height>=target_height and width>=target_width:\n",
        "            resized_vector = cv2.resize(vector, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "        elif height>=target_height and width<target_width:\n",
        "            resized_vector = cv2.resize(vector, (width, target_height), interpolation=cv2.INTER_AREA)\n",
        "        elif height<target_height and width>=target_width:\n",
        "            resized_vector = cv2.resize(vector, (target_width, height), interpolation=cv2.INTER_AREA)\n",
        "        else:\n",
        "            resized_vector = cv2.resize(vector, (width, height), interpolation=cv2.INTER_AREA)\n",
        "        # Calculate padding to reach target size\n",
        "        pad_width = target_width - resized_vector.shape[1]\n",
        "        pad_height = target_height - resized_vector.shape[0]\n",
        "        top, bottom = pad_height // 2, pad_height - (pad_height // 2)\n",
        "        left, right = pad_width // 2, pad_width - (pad_width // 2)\n",
        "        # Setting padding, i.e., adding zeros along height or width, which are less target_size\n",
        "        padded_vector = cv2.copyMakeBorder(resized_vector, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
        "        list_copy.append(padded_vector)\n",
        "    list_copy = np.array(list_copy)\n",
        "    return list_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E18a83HYA7j"
      },
      "outputs": [],
      "source": [
        "# Applies transformation on a given array of image vectors for data augmentation to avoid overfitting\n",
        "def apply_transformation(image_vectors, additional_records_size):\n",
        "    random_indices = np.random.choice(image_vectors.shape[0], size=additional_records_size, replace=False)\n",
        "    translated_random_vectors = image_vectors[random_indices]\n",
        "    for vector in translated_random_vectors:\n",
        "        # Apply random scaling\n",
        "        scaled_vector = vector * np.random.uniform(0.8, 1.3)\n",
        "        # Apply random translation\n",
        "        translated_vector = scaled_vector + np.random.normal(loc=0, scale=0.1, size=vector.shape)\n",
        "        # Apply random noise\n",
        "        noisy_vector = translated_vector + np.random.normal(loc=0, scale=0.05, size=vector.shape)\n",
        "        vector = noisy_vector\n",
        "    return translated_random_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63RzTKHLYA7k"
      },
      "outputs": [],
      "source": [
        "# Function for individual analysis of Widths and Heights of all the image vectors\n",
        "def image_vector_shape_analysis(image_shapes_list):\n",
        "    # Converting the given list into numpy array\n",
        "    im_shapes_array = np.array(image_shapes_list)\n",
        "    # Separating Widths and Heights of Images for individual analysis\n",
        "    im_shapes_height = [im_shapes_array[i][0] for i in range(len(im_shapes_array))]\n",
        "    im_shapes_width = [im_shapes_array[i][1] for i in range(len(im_shapes_array))]\n",
        "    sorted_shapes_height = im_shapes_height\n",
        "    sorted_shapes_width = im_shapes_width\n",
        "    sorted_shapes_height.sort()\n",
        "    sorted_shapes_width.sort()\n",
        "    # Individual analysis of Widths and Heights of all the image vectors\n",
        "    print(\"maximum height:\", max(im_shapes_height), \"maximum width:\", max(im_shapes_width))\n",
        "    print(\"minimum height:\", min(im_shapes_height), \"minimum width:\", min(im_shapes_width))\n",
        "    print(\"mean height:\", sum(im_shapes_height)/len(im_shapes_height), \"mean width:\", sum(im_shapes_width)/len(im_shapes_width))\n",
        "    print(\"median height:\", statistics.median(sorted_shapes_height), \"median width:\", statistics.median(sorted_shapes_width))\n",
        "    print(\"standard deviation of height:\", (statistics.variance(im_shapes_height))**0.5,\"standard deviation of width:\", (statistics.variance(im_shapes_width))**0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XrDTg6pYA7k"
      },
      "outputs": [],
      "source": [
        "# Function for comparing original image and resized image\n",
        "def compare_image_vectors(initial_vector_list, final_vector_list):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create a figure with two subplots\n",
        "    random_sample = random.randint(0, len(initial_vector_list) - 1)\n",
        "    # Get dimensions of the two images (in pixels)\n",
        "    vector1 = initial_vector_list[random_sample]\n",
        "    vector2 = final_vector_list[random_sample]\n",
        "    # Display the first image on the left subplot\n",
        "    axes[0].imshow(vector1)\n",
        "    axes[0].set_title(f'Original - {vector1.shape[0]}x{vector1.shape[1]}')  # Optionally set a title for the first image\n",
        "    # Display the second image on the right subplot\n",
        "    axes[1].imshow(vector2)\n",
        "    axes[1].set_title(f'Resized - {vector2.shape[0]}x{vector2.shape[1]}')  # Optionally set a title for the second image\n",
        "    plt.show()\n",
        "    return random_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYveOXecYA7k"
      },
      "outputs": [],
      "source": [
        "# Locating Datasets\n",
        "# Present in your local google colab repository\n",
        "gsg_src = \"/content/dataset/CMIL-Assessment/globally_sclerotic_glomeruli/\"\n",
        "ngsg_src = \"/content/dataset/CMIL-Assessment/globally_sclerotic_glomeruli/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAw_4CFeYA7k"
      },
      "outputs": [],
      "source": [
        "# For globally sclerotic glomeruli\n",
        "# Converting gsg images into Vectors\n",
        "gsg_im_vector, gsg_im_vector_shapes = image_vector(gsg_src)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del gsg_im_vector_shapes\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "cdfOmWt5kBoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0aCEyvpYA7k"
      },
      "outputs": [],
      "source": [
        "# Resizing the gsg image Vectors\n",
        "resized_gsg_im_vector = reshape_image_size(gsg_im_vector, 300, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dln60CNpYA7k"
      },
      "outputs": [],
      "source": [
        "# Data augmentation using Transformation on existing image vectors\n",
        "resized_gsg_im_vector_with_data_aug = apply_transformation(resized_gsg_im_vector, len(resized_gsg_im_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3WwJpi7YA7k"
      },
      "outputs": [],
      "source": [
        "# Final gsg vector along with data augmentation\n",
        "final_gsg_im_vector = np.concatenate((resized_gsg_im_vector, resized_gsg_im_vector_with_data_aug), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhUsrtTYA7k"
      },
      "outputs": [],
      "source": [
        "# Comparing Original Image Vectors (Globally Sclerotic images) with Resized Image Vectors\n",
        "sample = compare_image_vectors(gsg_im_vector, resized_gsg_im_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del gsg_im_vector\n",
        "del resized_gsg_im_vector\n",
        "del resized_gsg_im_vector_with_data_aug\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "wsel6ZCdiUoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del sample\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "_a6c_kY7vyg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbs-7UwpYA7l"
      },
      "outputs": [],
      "source": [
        "# Creating a list consisting the glomeruli type (1) and converting it into array.\n",
        "# Glomeruli of type = 1 means that it is Globally Sclerotic.\n",
        "gsg_glomeruli = [1 for i in range(len(final_gsg_im_vector))]\n",
        "gsg_glomeruli = np.array(gsg_glomeruli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LPw9hY-YA7l"
      },
      "outputs": [],
      "source": [
        "# For non globally sclerotic glomeruli\n",
        "# Converting Images into Vectors\n",
        "ngsg_im_vector, ngsg_im_vector_shapes = image_vector(ngsg_src)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del ngsg_im_vector_shapes\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "3k6losXCk2Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be7u4kbPYA7l"
      },
      "outputs": [],
      "source": [
        "# Resizing the Image Vectors\n",
        "resized_ngsg_im_vector = reshape_image_size(ngsg_im_vector, 300, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnFiRvdFYA7l"
      },
      "outputs": [],
      "source": [
        "# Data augmentation using Transformation on existing image vectors\n",
        "resized_ngsg_im_vector_with_data_aug = apply_transformation(resized_ngsg_im_vector, len(resized_ngsg_im_vector)//4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UqSJDv6YA7l"
      },
      "outputs": [],
      "source": [
        "# Final ngsg vector along with data augmentation\n",
        "final_ngsg_im_vector = np.concatenate((resized_ngsg_im_vector, resized_ngsg_im_vector_with_data_aug), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGn4wyDlYA7l"
      },
      "outputs": [],
      "source": [
        "# Comparing Original Image Vectors (Non Globally Sclerotic images) with Resized Image Vectors\n",
        "sample1 = compare_image_vectors(ngsg_im_vector, resized_ngsg_im_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del ngsg_im_vector\n",
        "del resized_ngsg_im_vector\n",
        "del resized_ngsg_im_vector_with_data_aug\n",
        "del sample1\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "fCGWXB8sk778"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKFf1jIWYA7l"
      },
      "outputs": [],
      "source": [
        "# Creating a list consisting the glomeruli type (0) and converting the list into array.\n",
        "# Glomeruli of type = 1 means that it is Non Globally Sclerotic.\n",
        "ngsg_glomeruli = [0 for i in range(len(final_ngsg_im_vector))]\n",
        "ngsg_glomeruli = np.array(ngsg_glomeruli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyO-sk5XYA7l"
      },
      "outputs": [],
      "source": [
        "# Concatenating the resized gsg image vector \"resized_gsg_im_vector\" and resized ngsg image vector \"resized_ngsg_im_vector\"\n",
        "combined_final_im_vector = np.concatenate((final_gsg_im_vector, final_ngsg_im_vector), axis=0)\n",
        "combined_final_glomeruli = np.concatenate((gsg_glomeruli, ngsg_glomeruli), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del final_gsg_im_vector\n",
        "del final_ngsg_im_vector\n",
        "del gsg_glomeruli\n",
        "del ngsg_glomeruli\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "mMSgwOxvlGEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiSTIU6nYA7l"
      },
      "outputs": [],
      "source": [
        "# Rearranging the order of the the image vectors and glomeruli type\n",
        "shuffled_indices = np.arange(combined_final_im_vector.shape[0])\n",
        "np.random.shuffle(shuffled_indices)\n",
        "combined_final_im_vector = combined_final_im_vector[shuffled_indices]\n",
        "combined_final_glomeruli = combined_final_glomeruli[shuffled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del shuffled_indices\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "yZbiSNkXwe1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us4vuaAQYA7l"
      },
      "outputs": [],
      "source": [
        "# Save the combined resized image vector along with combined glomeruli into a HDF5 file\n",
        "# If you want to save the data for future use\n",
        "# with h5py.File(\"Dataset.h5\", 'w') as hf:\n",
        "#             # Append data to the new .h5 dataset\n",
        "#             hf.create_dataset('image_vector', data=combined_final_im_vector_random)\n",
        "#             hf.create_dataset('glomeruli_type', data=combined_final_glomeruli_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFHnepy4YA7l"
      },
      "outputs": [],
      "source": [
        "# Accessing Data to train our deep learning Model, if the dataset was saved earlier\n",
        "# If have saved the data earlier, you have to implement this step\n",
        "# with h5py.File(\"Dataset.h5\", 'r') as hf:\n",
        "#     imv_feature = hf['image_vector'][:]\n",
        "#     glm_target = hf['glomeruli_type'][:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "combined_final_im_vector = combined_final_im_vector/255.0"
      ],
      "metadata": {
        "id": "-bvcYDrRtlHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsZRiybIYA7m"
      },
      "outputs": [],
      "source": [
        "# Stratified split to ensure balanced classes in both sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(combined_final_im_vector, combined_final_glomeruli, test_size=0.3, stratify=combined_final_glomeruli, random_state=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCglcLuqYA7m"
      },
      "outputs": [],
      "source": [
        "# Defining the input layer to process the image vectors\n",
        "input_layer = Input(shape=(300, 300, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x88_j5iYA7m"
      },
      "outputs": [],
      "source": [
        "# Loading the EfficientNetB3 model with no pre-trained weights.\n",
        "base_model = EfficientNetB3(include_top=False, input_tensor=input_layer, weights=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdtZPLuNYA7m"
      },
      "outputs": [],
      "source": [
        "# Adding a custome last layer, which is the output layer.\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output_layer = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.01))(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqtBHbIKYA7m"
      },
      "outputs": [],
      "source": [
        "# Creating the Initial Model.\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "# Freezing the base model layers Initially, to avoid any learning\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "# Compiling the Model.\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGSGRv9iYA7m"
      },
      "outputs": [],
      "source": [
        "# Setting main Hyperparameters for initial training\n",
        "batches = 16\n",
        "total_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkX0mQ7YYA7m"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "b3ic = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    epochs=total_epochs,\n",
        "    batch_size = batches,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBdR-1s4YA7m"
      },
      "outputs": [],
      "source": [
        "# Analyzing the performance (accuracy) of the trained model, over number of epochs.\n",
        "plt.plot(b3ic.history['loss'])\n",
        "plt.plot(b3ic.history['val_loss'])\n",
        "plt.title('b3ic Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some layers and fine-tune the model\n",
        "for layer in model.layers[:-10]:\n",
        "    layer.trainable = True\n",
        "# Compiling the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "i52eCmJRq4yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting main Hyperparameters for Fine tuning\n",
        "fine_tune_batches = 16\n",
        "fine_tune_epochs = 7"
      ],
      "metadata": {
        "id": "VdpvTJpiq6LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY961AHCYA7m"
      },
      "outputs": [],
      "source": [
        "# Re-training the Model\n",
        "fine_tuned_b3ic = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_test, y_test),\n",
        "    epochs=fine_tune_epochs,\n",
        "    batch_size = fine_tune_batches\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhQR94WvYA7m"
      },
      "outputs": [],
      "source": [
        "# Analyzing the performance (accuracy) of the fine tuned model, over epochs.\n",
        "plt.plot(fine_tuned_b3ic.history['loss'])\n",
        "plt.plot(fine_tuned_b3ic.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6FY7s6ZYA7m"
      },
      "outputs": [],
      "source": [
        "# Predicting the output on test dataset, to evaluate the performance(using metrics such as Accuracy, Precision, Recall, etc).\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kfz8v5ZYA7m"
      },
      "outputs": [],
      "source": [
        "# Converting every predicted output into 0s if the output value is less than 0.5, else 1 otherwise.\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating precision and recall.\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "# Computing confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Y1f9yAsjaPg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Precision: {precision}\\nRecall: {recall}\\nConfusion Matrix:\\n {conf_matrix}')"
      ],
      "metadata": {
        "id": "jvgkOWMPaW-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Heatmap, to show Confusion Matrix.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "soY2zIKqauLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting variables for RAM Optimization\n",
        "del x_train\n",
        "del x_test\n",
        "del y_train\n",
        "del y_test\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ikpXO_fjrSKR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 5067435,
          "sourceId": 8493248,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}